{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Embedding Tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What you can expect from this tutorial\n",
    "At the end of this notebook, you'll have an understanding of\n",
    "- what is a word vector, and why do I need one?\n",
    "- how to get word vectors\n",
    "    - distributional semantics: simple counts\n",
    "    - word embeddings: neural methods\n",
    "- how to use word embeddings\n",
    "    - finding similar terms\n",
    "    - combining concepts and searching for terms between them, \"more like me\"\n",
    "    - using negative examples to complete analogy questions (sometimes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What's out-of-scope for this tutorial?\n",
    "You won't learn about\n",
    "- different methods for training neural word embeddings\n",
    "    - CBOW/Skip-gram\n",
    "    - Word2Vec\n",
    "    - How FastText works\n",
    "- Using word embeddings for downstream classification tasks\n",
    "- Measuring the goodness of your embedding model\n",
    "    - detecting unethical bias in your model\n",
    "    - comparing one word embedding model to another to determine which is better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's start a dictionary so we can store definitions for words we don't know\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why do we need word vectors?\n",
    "Remember feature vectors? We have this list of numbers that represents an instance of data. This numerical representation allows us to make predictions using predictive modeling algorithms. \n",
    "\n",
    "Consider the feature vectors for cars.\n",
    "\n",
    "|   instances   | is_red      | has_4_wheel_drive | label |\n",
    "| ------------- | ----------- | ----------- | ---------|\n",
    "| instance_1    | 0           | 1 | truck |\n",
    "| instance_2    | 1           | 0 | car |\n",
    "\n",
    "A word vector is a way to represent the features of this word and (maybe) use in a downstream modeling task. Word vectors allow us to represent the meaning of a word numerically. With this vector represation of a word's semantics, we can easily give this information to predictive models.\n",
    "\n",
    "A word vector might look like this\n",
    "\n",
    "|   words   | appeared_near_yum      | appeared_near_red |\n",
    "| ------------- | ----------- | ----------- |\n",
    "| apple    | 1           | 1 |\n",
    "| applesauce    | 1           | 0 |\n",
    "| firetruck | 0 | 1 |\n",
    "\n",
    "This is in accordance with the hypothesis of [distributional semantics](https://en.wikipedia.org/wiki/Distributional_semantics#:~:targetText=Distributional%20semantics%20is%20a%20research,large%20samples%20of%20language%20data.): _linguistic items with similar distributions have similar meanings_\n",
    "\n",
    "A _word vector_ uses the distributional hypothesis. Each word vector represents a word with a very high-dimensional sparse vector, where each dimension reflects a context in which the word occurred in the corpus. For example, a context could be another word that appeared in proximity.\n",
    "\n",
    "A _word embedding_ represents a word with a low-dimensional vector (e.g. 100 dimensions). The dimensions are usually latent, and often obtained using the information as in the distributional semantics approach (e.g. LSA, word2vec). ([source](https://www.quora.com/Whats-the-difference-between-word-vectors-word-representations-and-vector-embeddings))\n",
    "\n",
    "Word embeddings are often used as a feature extraction technique. But the embeddings themselves are useful too!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# update the definition dictionary with new terms \n",
    "# distributional hypothesis, distrubtional semantics, word vector, word embedding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you may want these imports for the next coding section\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# implement this simple way to make a word vector using a corpus\n",
    "'''\n",
    "----------------------------------------------\n",
    "Objective: make word vectors using context\n",
    "----------------------------------------------\n",
    "create a function that takes a file of sentences and returns\n",
    "a dictonary mapping each word in the corpus to its vector\n",
    "```\n",
    ">>> vectors = make_simple_word_vectors(filename)\n",
    ">>> vectors['dog']\n",
    "np.array([0., 0., 0., 0., 0., 0., 2., 0., 0., 0., \n",
    "         0., 0., 0., 1., 1., 0., 4., 0., 0., 0.])\n",
    "```\n",
    "To make sure everyone has the same columns, use a helper function,\n",
    "`make_vocab(filename)` which returns a map of the vocab item to the index\n",
    "of the column it corresponds to. Sort the vocab so that the index of 'a' is 0\n",
    "```\n",
    ">>> vocab = make_vocab(filename)\n",
    ">>> vocab['a']\n",
    "0\n",
    ">>> vocab['dog']\n",
    "9\n",
    "```\n",
    "You're welcome to use other helper methods.\n",
    "\n",
    "For your corpus, use the file: data/10_sentences.txt\n",
    "'''\n",
    "\n",
    "def make_vocab(filename):\n",
    "    \"\"\"\n",
    "    :param filename: the location fo the file with the word data to build vectors from\n",
    "    :return: dictionary mapping each vocab item to it's index\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "def make_simple_word_vectors(filename, window_size=2):\n",
    "    \"\"\"\n",
    "    :param filename: the location of the file with word data to build vectors from\n",
    "                     the file should be formatted with each sentence on a line\n",
    "    :param window_size: how far to the left and right of a word to look for context\n",
    "                        for window_size=2, we look 2 words to the left, and 2 words right\n",
    "    :return: dictionary mapping a word to its vector\n",
    "             the columns in the vector should be ordered alphabetically\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "# always debug with a small file first\n",
    "small_file = os.path.join('data', '10_sentences.txt')\n",
    "vocab = make_vocab(small_file)\n",
    "assert(len(vocab) == 20)\n",
    "\n",
    "word_to_vector_map = make_simple_word_vectors(small_file)\n",
    "assert(np.array_equal(word_to_vector_map['dog'],\n",
    "                      np.array([0., 0., 0., 0., 0., 0., 2., 0., 0., 0., \n",
    "                                0., 0., 0., 1., 1., 0., 4., 0., 0., 0.])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We've incorporated a word's context into the feature vector\n",
    "> _\"You shall know a word by the company it keeps.\" - John Rupert Firth_\n",
    "\n",
    "We need the word vector to capture the similarity between words. From linguistics, a simple approach is to say that words that fall in similar contexts have similar meanings. The word vectors we've created capture this notion of contextually similar terms. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pros and Cons\n",
    "\n",
    "Pros\n",
    " - captures the context of words\n",
    " \n",
    "Cons\n",
    " - [homographs](https://en.wikipedia.org/wiki/Homograph) have the same vector\n",
    " - [sparse vectors](https://en.wikipedia.org/wiki/Sparse_matrix) are expensive\n",
    "     - could do a dimensionality reduction technique like [SVD](https://en.wikipedia.org/wiki/Singular_value_decomposition)\n",
    "     \n",
    "In practice, the sparsity of the vectors on large corpora make using them impossible. \n",
    "(That's why we only used 10 sentences.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# update dictionary of new terms\n",
    "# homograph, sparse vector\n",
    "# don't worry about SVD - not important for what we're doing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Word Embeddings\n",
    "In practice, the count method can be improved upon, both computationally and theoretically.\n",
    "\n",
    "Instead of counting up words near other words, let's use a neural network to get word embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_But... don't we need labeled data?_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a form of [semi-supervised learning](https://en.wikipedia.org/wiki/Semi-supervised_learning#:~:targetText=Semi%2Dsupervised%20learning%20is%20a,large%20amount%20of%20unlabeled%20data.), which makes use of unlabeled data for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# update dictionary of new terms\n",
    "# semi-supervised learning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_So... what are we predicting then?_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's hide one of the words in a data instance.\n",
    "\n",
    "> I need to feed my ____ today\n",
    "\n",
    "Humans have no problem knowing what kinds of words might fill this blank, and knowing words that do _not_ fit in this blank. Let's train a neural network to predict this missing word given the context. This method is called CBOW, and we'll go into more depth in day 2. \n",
    "\n",
    "![CBOW](figures/word2vec-cbow.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Okay, great, we have this model that fills in the blanks. How does that get us word vectors?_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the Weight matrix W. Each row in this weight matrix corresponds to a word in the vocab from the corpus a model was trained on.\n",
    "\n",
    "![weight_matrix](figures/weight-matrix.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello_embedding = [-3.2  -0.89  0.89  0.19]\n"
     ]
    }
   ],
   "source": [
    "# say we have a weight matrix W and a one-hot encoding for \"hello\"\n",
    "W = np.array([[.47, .56, .98, -.23],\n",
    "              [-.21, -.79, -.99, 1.2],\n",
    "              [-3.2, -.09, .09, .19],\n",
    "              [-3.2, -.89, .89, .19],\n",
    "             ])\n",
    "hello = np.array([0, 0, 0, 1])\n",
    "# QUIZ: how many words are in the vocab?\n",
    "# to get the word embedding for hello, we need to multiply\n",
    "hello_embedding = np.matmul(hello, W)\n",
    "print(f\"hello_embedding = {hello_embedding}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's implement a neural word embedding model using [FastText](https://arxiv.org/pdf/1607.04606.pdf). \n",
    "\n",
    "But first, we need to implement something to preprocess the data more efficiently. Holding all the data in memory at once may lead to a memory error. Instead we want to stream the data, only looking at one line in the file at a time.\n",
    "\n",
    "### Iterators and Generators in Python\n",
    "Python cares more about how objects behave rather than what they are. \n",
    "\n",
    "For instance, you can iterate through a list, and you can iterate through a dictionary's keys and values. These objects are _iterable_. They have a class method `__iter__()` and can be called like `for x in iterable:`. \n",
    "\n",
    "![Generators and Iterators](figures/iterators.png)\n",
    "\n",
    "[Reference](https://nvie.com/posts/iterators-vs-generators/)\n",
    "\n",
    "Generators are iterators, but instead of holding every line in memory, they compute it on the fly. They still have an `__iter__()` class method and work in `for x in generator:` calls, but they are much less memory intensive. \n",
    "\n",
    "Generators make use of the `yield` keyword, which then returns to the outer call of iteration with only that line. When the iterable needs to call the next item, it returns to the line immediately after the yield and continues. \n",
    "\n",
    "If a `return` statement is present in a generator function, then a `StopIteration` is raised, and the iteration is halted. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generator examples\n",
    "\n",
    "def my_generator():\n",
    "    yield 1\n",
    "    yield 2\n",
    "    \n",
    "for x in my_generator():\n",
    "    print(f\"x={x}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "nums = my_generator()\n",
    "\n",
    "for x in nums:\n",
    "    print(f\"x={x}\")\n",
    "    \n",
    "for x in nums:\n",
    "    print(f\"x={x}\")\n",
    "    \n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Quiz: Why didn't it go through twice?\n",
    "\n",
    "For this reason, we'll create a class that has an `__iter__()` method that we'll override. This way, every time we call `for sent in sentences:`, we'll have data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See Day 2's tutorial for more algorithms to train word embeddings\n",
    "# for more on different training algorithms\n",
    "\n",
    "'''\n",
    "-------------------------------------------------------\n",
    "Objective: create a generator for better performance\n",
    "-------------------------------------------------------\n",
    "first, we need to preprocess the data. \n",
    "\n",
    "The input to FastText is an iterable of sentences \n",
    "where a sentence is formatted as: \n",
    "`['this', 'is', 'a', 'sample', 'sentence']`\n",
    "\n",
    "So you could use a list of sentences \n",
    "```\n",
    "sentences = [\n",
    "    ['this', 'is', 'a', 'sample', 'sentence'],\n",
    "    ['this', 'is', 'another', 'sample', 'sentence'],\n",
    "]\n",
    "```\n",
    "but holding all of the corpus's sentences in memory at once is expensive.\n",
    "You might hit a memory error.\n",
    "\n",
    "Instead, let's use a generator that yields data on the fly.\n",
    "\n",
    "Create a class `Sentences` that can be iterated through to train a model.\n",
    "Make sure your object yields preprocessed data. \n",
    "That might mean all lowercase, no punctuation, but you can make your own choices\n",
    "with what you think will work best.\n",
    "\n",
    "```\n",
    ">>> corpus = os.path.join('data', 'got_scripts_breakdown.csv')\n",
    ">>> sentences = Sentences(filename=corpus,\n",
    "                          delim=';',\n",
    "                          encoding='utf-8-sig',\n",
    "                          limit=5,\n",
    "                          )\n",
    ">>> for sent in sentences:\n",
    "        print(sent)\n",
    "['what', 'd’you', 'expect', 'they’re', 'savages', 'one', 'lot', 'steals', 'a', 'goat', \n",
    " 'from', 'another', 'lot', 'and', 'before', 'you', 'know', 'it', 'they’re', 'ripping', \n",
    " 'each', 'other', 'to', 'pieces']\n",
    "['i’ve', 'never', 'seen', 'wildlings', 'do', 'a', 'thing', 'like', 'this', 'i’ve', \n",
    " 'never', 'seen', 'a', 'thing', 'like', 'this', 'not', 'ever', 'in', 'my', 'life']\n",
    "['how', 'close', 'did', 'you', 'get']\n",
    "['close', 'as', 'any', 'man', 'would']\n",
    "['we', 'should', 'head', 'back', 'to', 'the', 'wall']\n",
    "```\n",
    "'''\n",
    "\n",
    "import string\n",
    "\n",
    "class Sentences:\n",
    "    def __init__(self, filename, delim, encoding, limit=float('inf')):\n",
    "        self.filename = filename\n",
    "        self.delim = delim\n",
    "        self.encoding = encoding\n",
    "        self.limit = limit\n",
    "        \n",
    "    def __iter__(self):\n",
    "        pass\n",
    "                \n",
    "\n",
    "got_dialogue_file = os.path.join('data', 'got_scripts_breakdown.csv')\n",
    "sents = Sentences(filename=got_dialogue_file, \n",
    "                  delim=';', \n",
    "                  encoding='utf-8-sig',\n",
    "                  limit=5)\n",
    "for sent in sents:\n",
    "    print(sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great, our data is preprocessed, and now we're ready to train a model.\n",
    "Check out [gensim's documentation](https://radimrehurek.com/gensim/models/fasttext.html) too for more information on the hyperparameters you can tune while training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "-------------------------------------------------\n",
    "objective: train a FastText word embedding model\n",
    "-------------------------------------------------\n",
    "use gensim to train a model. Follow the documentation for an example\n",
    "'''\n",
    "\n",
    "from gensim.models import FastText\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What's happening when we train?\n",
    "\n",
    "We then iterate through the data, to build the vocabulary. This determines which words are frequent enough to build vectors for, which words will be included in the model. \n",
    "\n",
    "At the start of training the model, the weights for the word embeddings are randomly initialized. Then the model iterates through the data, making predictions. \n",
    "\n",
    "![Prediction/Inference Step](figures/nn-inference.gif)\n",
    "\n",
    "Then we evaluate how far off we were using the cost function.\n",
    "![Calculating the loss](figures/nn-cost.gif)\n",
    "\n",
    "After training completes, we take the weight matrix. Each row corresponds to a word in our vocabulary. These vectors live in 300 dimensional space!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you can accesss the vectors from the model using model.wv\n",
    "# and keying in like a dictionary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what about words that aren't in the corpus?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What can we do with word embeddings?\n",
    "\n",
    "Similar vectors imply semantically similar concepts. Vectors that live in the same space mean very similar things. Let's use the similarity of vectors to find similar words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What makes a vector similar to another one?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let me show you. Our vectors had a large dimension (100? 300?), but that's difficult to visualize. We can use an algorithm like [t-SNE](https://en.wikipedia.org/wiki/T-distributed_stochastic_neighbor_embedding) to visualize in 2 or 3 dimensions more easily. \n",
    "\n",
    "Here's an example of word embeddings that have undergone t-SNE.\n",
    "![word embeddings plotted](figures/word-embedding-clusters.png)\n",
    "\n",
    "Words with similar meanings are close together in space! We lose some information in the dimension reduction to plot in 2d space -- which may be why the months are shown in two different places, but there may be another more meaningful reason."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Okay, but how can I tell which things are close without that loss of information from the dimension reduction? I don't need a picture, I just want numbers._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to use a distance [metric](https://en.wikipedia.org/wiki/Metric_(mathematics)#:~:targetText=In%20mathematics%2C%20a%20metric%20or,of%20elements%20of%20a%20set.&targetText=A%20metric%20induces%20a%20topology,a%20metric%20is%20called%20metrizable.). The closer two vectors are, the more similar their meanings tend to be. \n",
    "\n",
    "Here's a dirty secret topologists won't tell you: You can define distance however you want to. There's no universal distance metric that works well in all spaces, on all data sets, or on all concepts.\n",
    "\n",
    "Let's talk about two commonly used distance metrics.\n",
    "\n",
    "![vector similarity](figures/The-difference-between-Euclidean-distance-and-cosine-similarity.png)\n",
    "\n",
    "| Metric        | pro           | con   |\n",
    "| ------------- |-------------| -----|\n",
    "| euclidean     | distance ranges from \\[0, inf\\]             | takes the magnitude of the vectors into account |\n",
    "| cosine        | captures difference in direction of vectors | bounded \\[0,2\\] |\n",
    "\n",
    "In practice, cosine similarity _usually_ gives more insightful results than euclidean, likely because of the issues with vectors of significantly different magnitudes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# update new terms dictionary \n",
    "# distance metric\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gensim builds in this functionality for you! Checkout model.wv.most_similar()\n",
    "# they use cosine similarity, a score between 0 and 1, 1 meaning very similar\n",
    "\n",
    "# CAUTION: there's also cosine _distance_ which returns a score between 0 and 2 \n",
    "#          where 0 means most similar. Always double check your cosine function\n",
    "# we've checked for you: gensim uses cosine similarity\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# we can even plot these vectors in space using t-SNE\n",
    "# we've implemented this function for you for the sake of time\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import os\n",
    "from gensim.models import FastText\n",
    "import pandas as pd\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "def plot_word_vectors(word_lists, model):\n",
    "    all_words = [word for words in word_lists for word in words]\n",
    "    \n",
    "    X = [model.wv[word] for word in all_words]\n",
    "    tsne = TSNE(n_components=2)\n",
    "    X_tsne = tsne.fit_transform(X)\n",
    "    \n",
    "    df = pd.DataFrame(X_tsne, index=all_words, columns=['x', 'y'])\n",
    "    \n",
    "    dfs = [df.loc[words] for words in word_lists]\n",
    "    \n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(1,1,1)\n",
    "    \n",
    "    colors = cm.rainbow(np.linspace(0,1, len(dfs)))\n",
    "    for df, color in zip(dfs, colors):\n",
    "        ax.scatter(df['x'], df['y'], color=color)\n",
    "    \n",
    "    for df in dfs:\n",
    "        for word, row in df.iterrows():\n",
    "            plt.text(row['x'], row['y'], word)\n",
    "    plt.show()\n",
    "\n",
    "words = [\n",
    "    [word for word, _ in model.wv.most_similar('winter', topn=20)],\n",
    "    [word for word, _ in model.wv.most_similar('dragon', topn=20)],\n",
    "    [word for word, _ in model.wv.most_similar('wedding', topn=20)],\n",
    "]\n",
    "plot_word_vectors(words, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What is lost with a dimensionality reduction technique?\n",
    "\n",
    "Think about looking at this data set from different angles. \n",
    "![3d data](figures/3d-scatter-plot-rotate.gif)\n",
    "\n",
    "When you're viewing high dimensional data in lower dimensions, you don't get the best idea for what's really happening. But it can be insightful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Searching for more than one word at a time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gensim also allows you to search for multiple concepts\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How does it know what to pull back for more than one word?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It takes an average of the vectors provided in the list of positive objects. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "v1: [ 3.72374122 -3.3335414 ]\n",
      "v2: [-2.02106202  1.15256205]\n",
      "avg_v: [ 0.8513396  -1.09048967]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# let's show this in 2 dimensions\n",
    "v1 = np.random.uniform(low=-5, high=5, size=2)\n",
    "v2 = np.random.uniform(low=-5, high=5, size=2)\n",
    "print(f\"v1: {v1}\")\n",
    "print(f\"v2: {v2}\")\n",
    "\n",
    "avg_v = np.average([v1, v2], axis=0)\n",
    "print(f\"avg_v: {avg_v}\")\n",
    "\n",
    "V = [v1, v2, avg_v]\n",
    "\n",
    "# let's plot these vectors using matplotlib\n",
    "plt.quiver(v1[0], v1[1], color='r', scale=21)\n",
    "plt.quiver(v2[0], v2[0], color='b', scale=21)\n",
    "plt.quiver(avg_v[0], avg_v[1], color='g', scale=21)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Does the average vector always look like it is in the right spot? \n",
    "\n",
    "There may be a better function to use, but gensim implemented it this way. \n",
    "\n",
    "When two vectors are close together, this functionality makes a lot of sense. Find the word most similar to the average of these two words. But when two vectors are pointing in opposite directions, the average would be the origin, something kind of far away from both of those vectors. When giving examples, make sure your examples are similar enough to get reasonable results. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gensim also offers the ability to include negative examples.\n",
    "\n",
    "In fact, there are many papers publishing results from training word embeddings with such fantastic examples as `\"king\" - \"man\" + \"woman\" ≈ \"queen\"`. Let's see how this work on our data set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# implement the analogy by using 'king' and 'woman' in the positive params\n",
    "# and 'man' in the negative param\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This result is amazing (and famous).\n",
    "\n",
    "![analogies](figures/king-queen-analogy.png)\n",
    "\n",
    "But does it always work?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('aryas', 0.5778557062149048),\n",
       " ('jaimes', 0.4864669442176819),\n",
       " ('glancing', 0.40133431553840637),\n",
       " ('sansa', 0.39033061265945435),\n",
       " ('sansas', 0.37027913331985474),\n",
       " ('jaw', 0.36505961418151855),\n",
       " ('t', 0.35723018646240234),\n",
       " ('baelishs', 0.3550487756729126),\n",
       " ('baelish', 0.34758543968200684),\n",
       " ('screamer', 0.3465248942375183)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# try a couple other analogies to see if they work!\n",
    "# here's one to get you started \n",
    "model.wv.most_similar(positive=['jaime', 'arya'], negative=['lannister'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# can you find an analogy that works?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not as much luck in other analogy examples.\n",
    "\n",
    "This [paper](https://kawine.github.io/blog/nlp/2019/06/21/word-analogies.html) proves the conditions necessary for these incredible analogies to work. In short, more data likely solves this problem. \n",
    "\n",
    "![When analogies work](figures/when-analogies-work.png)\n",
    "\n",
    "\n",
    "In research, we've tried to figure out the best way to incorporate negative examples into our searches, but we just haven't found a consistently useful result. We also aren't using our word embeddings for analogies, so we don't want to get too hung up on this. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's save our model for further use\n",
    "# create the directory 'models'\n",
    "# and save the model as 'got_ft.model'\n",
    "\n",
    "save_path = os.path.join('models', 'got_ft.model')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other values of word embeddings coming from automatically transcribed speech data\n",
    "\n",
    "- we capture aliases! \n",
    "- we get tons of insight from the specific context of call centers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary - What did we learn today?\n",
    "- ways of training word embeddings\n",
    "    - raw counts from the data\n",
    "        - can't handle out-of-vocab words\n",
    "        - captures contextually similar words\n",
    "    - neural methods: FastText\n",
    "        - captures character similarities as well as contextual similarities\n",
    "- how to use word embeddings\n",
    "    - find similar words\n",
    "    - search for more than one word at a time\n",
    "    - search for \"a\", and \"b\", but not like \"c\" with questionable results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's review our dictionary of new terms\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Survey !!!\n",
    "\n",
    "Please complete the [course survey](https://forms.office.com/Pages/ResponsePage.aspx?id=gwv7BWBlfUGFbTjusOst_QYpnoW2nrtJmgVZLQ3gu25UMURGMDdaUTA0QUhJQTM3NlMxNE9GVVkyRC4u)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
