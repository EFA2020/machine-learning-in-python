{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Welcome to the Machine Learning in Python Tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What's in scope for this morning?\n",
    "The machine learning pipeline for predictive modeling\n",
    "1. Exploratory data analysis\n",
    "2. Preprocessing data for ML\n",
    "3. Training a model\n",
    "    1. Logistic Regression\n",
    "    2. MultiLayer Perceptron (NN)\n",
    "4. Using a model for predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What's _not_  in scope this morning?\n",
    "1. Different machine learning packages (TensorFlow, Keras, PyTorch, MXNet)\n",
    "2. Image Recognition\n",
    "3. Unsupervised learning (word embeddings, clustering, text generation)\n",
    "4. That one blog you read about sheep and self-driving cars"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Python Libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Python libraries are collections of functions and methods that allow you to avoid \"reinventing the wheel\". The more you use python, the more familiar you become with the libraries that are available. Throughout today, we will be accessing numerous libraries by first importing them and then referencing their methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the current working directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a vector of zeros of length 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# roll the dice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that if i keep running the above cell, jupyter knows that `randint()` is already imported and does not keep re-importing it.\n",
    "\n",
    "It is best practice to put all imports at the begining of a file when not using notebooks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# No compiling allowed: Python as a scripting language\n",
    "\n",
    "One of the cool things about Python is that it supports many (if not all) types of programming. Despite Python being known as a \"scripting language\", it is object-oriented. When doing complex machine learning, we heavily rely on objects to store and manage the multitude of data, model parts, and results. This morning, we will be doing a lot of scripting and procedural programming, but we will also be using objects from the python library (i.e dataframes). In the afternoon (and those joining us for Day 2), we will focus more on creating our own objects. While working today, keep in mind that all of this code could be written in a more object-oriented fashion based on the needs of the program."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is Machine Learning?\n",
    "The collection of algorithms that train models that can make predictions, create a vector space, or other useful structure. We do not tell the models what to do, they learn through trial and error\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is AI?\n",
    "At Callminer: How to market machine learning ;)\n",
    "\n",
    "In the real world: Machine Learning is a subset of Artifical Intelligence. According to DeepAI, AI is \n",
    ">\"intelligence of machines and computer programs, versus natural intelligence, which is intelligence of humans and animals.\"\n",
    "\n",
    "AI is the process of using algorithms to solve tasks. This includes rules based approaches as well as ML algorithms.\n",
    "\n",
    "\n",
    "![https://interestingengineering.com/whats-the-difference-between-machine-learning-and-ai](figures/ai_vs_ml.jpg)\n",
    "\n",
    "image from [here](https://interestingengineering.com/whats-the-difference-between-machine-learning-and-ai)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Learn more about [ai vs machine learning](https://towardsdatascience.com/clearing-the-confusion-ai-vs-machine-learning-vs-deep-learning-differences-fce69b21d5eb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Pipeline\n",
    "\n",
    "To get a better understanding of where we are doing, let's look at the machine learning pipeline.\n",
    "The pipeline for classification tasks in machine learning is:\n",
    "\n",
    "1. Define the problem\n",
    "2. Gather Data\n",
    "3. Exploratory Data Analysis\n",
    "4. Clean the Data\n",
    "5. Engineer Additional Features\n",
    "6. Prepare Features for ML\n",
    "7. Train a Model\n",
    "8. Predict\n",
    "9. Refine the model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the Problem\n",
    "\n",
    "Given information about a particular Game of Thrones episode and a line of dialogue, predict who said it.\n",
    "\n",
    "\n",
    "\n",
    "For example: In Season 6, Episode 5, who said \"Hold the Door\"?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gather Your Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \"Transcripts\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normally, when we make models here at CallMiner, we focus heavily on the transcripts of calls. These transcripts are broken into turns that contain turn-specific information, like who said it and when it was said. Unfortunately, we don't have quality call data to use as examples today. However, we do have Game of Thrones dialogue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dataframe from a csv of data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although we are often looking at calls on a turn basis, it helps to add information about the overall call in the models. For example, if we are prediciting whether or not a turn contains PII, it could be helpful to know which department took the call. Therefore, we also need call-level metadata. For our Game of Thrones example, this would be episode-level metadata."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Task_ : create a dataframe named `metadata` of episode meta data located as `data\\got_csv.csv`. Let the index_col be set to default. Display the first season of metadata.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# add code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we want to join the two dataframes so that each dialogue turn contains the episode metadata for which it was spoken."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# joing the two dataframes to create one dataframe, `data`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Discussion*: What do you notice about `data`? Why did it do that? How could it have been avoided?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Features and Instances\n",
    "\n",
    "From this point forward, we will be thinking of our dataframe in terms of *instances* and *features*. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Features\n",
    "Each column represents a feature. [Wikipedia](https://en.wikipedia.org/wiki/Feature_(machine_learning)) defined it the best :\n",
    "> In machine learning and pattern recognition, a feature is an individual measurable property or characteristic of a phenomenon being observed. Choosing informative, discriminating and independent features is a crucial step for effective algorithms in pattern recognition, classification and regression. \n",
    "\n",
    "We will work with *numerical*, *categorical*, and *text* features:\n",
    "\n",
    "\n",
    "1. Numerical features: features that are numbers\n",
    "    1. Discrete: numerical values that are integers (age in years)\n",
    "    2. Continuous: numerical values that can be any type of number (velocity)\n",
    "    \n",
    "2. Categorical features: feature value is chosen from a small group of possible options (college major)\n",
    "\n",
    "3. Text features: these features include strings that are extremely unique to a given instance (summary)\n",
    "\n",
    "The features tell us everything we know about the data we are predicting on. \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instances\n",
    "\n",
    "Each row represents an instance. An instance is a singular unit of data. Our goal is that the model will be able to predict the correct label given the features from the instance. In our example, each line of spoken dialogue is a unique instance and each instance has a value for each feature."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exploratory Data Analysis is the process of analyzing your data in order to understand its main characteristics. It's like:\n",
    "1. Reading the ingredients before taking a bite of a mystery nugget. \n",
    "2. Running a background check before a blind date\n",
    "\n",
    "\n",
    "It goes back to knowing what you are getting yourself into before you dive in"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Survey the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What even are the features?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What are run times?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# not per instance, per episode!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What are the unique run times?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how many times do each of these runtimes occur?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many times does someone say \"you know nothing\"?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Task*: Explore other fields in the data. What are the values? How often do they occur? Is anything weird happening?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add code here!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although counts and lists are useful, it is often helpful to look at how features are distributed. Python has a variety of plotting packages to help with this task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot a histogram of runtimes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot a count plot of episodes per director"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot a line graph of IMDB ratings for each episode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task: Pick some interesting features, and visualize their distribution! Anything strange? Anything cool? Find any correlations?\n",
    "(Pro Tip: pyplot and seaborn have gremlins. For time constraints, try to stick with one of the sample graphs we've provided! However here is the documentation for [matplot lib](https://matplotlib.org/gallery/index.html) and [seaborn](https://seaborn.pydata.org/examples/index.html) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add graph here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean the Data\n",
    "if you've asked us anything about how much data is enough, we almost always respond \"as much as you can get!\". However, the data needs to be as \"clean\" as possible. What does that even mean?\n",
    "Clean data:\n",
    "1. Not dirty\n",
    "\n",
    "So what is dirty data: \n",
    "1. Missing values\n",
    "2. Inaccurate values\n",
    "3. Irrelevant information\n",
    "4. It is in a format that machine learning algorithms won't accept\n",
    "\n",
    "Although some machine learning algorithms are great at sifting through the garbage to find important features, it's nice if we help along the way. A good thought to keep in mind: \"Would knowing this information help _me_ predict the answer?\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Marie Kondo the features --> What brings you joy?\n",
    "\n",
    "There are certain types of features that we like as data scientists. These features are easy for algorithms to use OR it is easy for us to transform them into formats algorithms can use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "#### Features that bring me joy?\n",
    "1. RELEVANT continuous and discrete data \n",
    "    1. heights in cm\n",
    "    2. distances\n",
    "    3. words per minute\n",
    "    4. NPS score\n",
    "2. Categorical data with a reasonable number of categories\n",
    "    1. state\n",
    "    2. agent\n",
    "    3. reason for call\n",
    "        \n",
    "#### Features that do not bring me joy?\n",
    "1. Unprocessed text\n",
    "    1. Transcripts\n",
    "    2. Comments\n",
    "2. Redundant features\n",
    "    1. DOB and Birth_date\n",
    "    2. Product Name and Product ID\n",
    "3. Categorical features with too many unique values\n",
    "    1. ANI\n",
    "    2. Last Name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[What makes a good feature-video](https://www.youtube.com/watch?v=N9fDIAflCMY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task: Look at your columns and create a list of column names for the columns that do not bring you joy. Assign this list to the variable `to_be_removed`. Do NOT remove `Sentence` nor `Name`. We need them later.\n",
    "\n",
    "Ex: `to_be_removed = [\"ANI\", \"Last Name\", \"DOB\"...]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add list here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# if you like keeping a back_up before deleting\n",
    "# Note: if something does get messed up, you can always rerun all the cells from the beginning\n",
    "back_up_data_1 = data.copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove columns that contain bad features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fix Inconsistencies\n",
    "Until the robots finally take our jobs, much of data collection and data entry is still done by hand. And WE'RE ONLY HUMAN! (I can't imagine how many typos and syntax errors have already occured by now if you're experiencing this live). Therefore, data can be inconsistent-- whether it be spelling errors, missing values, changes in format, etc. Before creating a model, it is important that we find any inconsistencies with our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Who were the unique writers on Game of Thrones?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " We want our model to recognize a unique list of writers (Cogman, Hill, Weiss, Benioff, Martin, Espenson, Taylor). Instead of seing 'David Benioff & D.B Weiss' as one new author, we want it to recognize that it is author 'David Benioff' AND author 'D.B. Weiss'. We aren't going to fix this right now BUT we will fix it very soon. Don't forget about it!\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Task_ : What other important column is full of typos? Find it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are at least 4 ways that `alliser thorne` is referenced but that is the same person! Not good if our model is trying to predict who said a particular line. We don't want the model deciding between `alliser thorn` and `alliser thorne`! Therefore, we need to fix these so there is one unique name for each character.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace `alliser thorn` with `alliser thorne` "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we would have to do: find all the instances where a character was referenced different ways and change it to one reference\n",
    "\n",
    "What I did for you: that.\n",
    "\n",
    "Earlier, we mentioned that you can import python libraries, which are packages of code. Turns out you can also import methods and objects that you have in other files. This makes data cleaning much easier because once you write it once, you can reuse it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import NamesToReplace object, get ntR.names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Task_ : Write code that loops through the names in `nTR` and replaces them in `data` as we did with allister. How many characters do we have left?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We still have a TON of characters, many who don't say much"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# graph distribution of counts of characters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a ton of characters who don't say hardly anything and then it is pretty even. We want to count those characters as \"other\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    " # Get a list of all characters who spoke less than 30 times"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Task_ : Replace all those character names with \"other\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Who's left?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing data for ML \n",
    "\n",
    "Features must be represented in a way that a computer can understand and _learn_ from. We do this through one-hot coding and normalizing.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-hot Encodings of Categorical Data\n",
    "\n",
    "One-hot encoding is how we turn categorical data into zeros and ones.\n",
    "![one-hot-encoding](figures/one_hot_encoding.png)\n",
    "\n",
    "One-hot encoding is necessary because the math used in machine learning algorithms requires all features to be numbers. One-hot coding provides a way to express membership within a group and lack of membership in the others."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you see above, categorical features should be one-hot encoded. But when should discrete data be one-hot encoded? Let's think about the column \"N_Season\".... Does the fact that an episode is in season 4 mean that it is more _something_ than an episode in season 2? Does the fact that an episode had 4.4 million views tell you that it is more _something_ than an episode with 2.2 million views? If there is a relationship with how the discrete value increases or decreases, it should be treated like a number. If there is not, it should be one-hot-encoded."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Task_ : Create a list of column names for the columns that should be one-hot encoded. Assign this list to the variable `one_hots`. Ignore `Name` and `Sentence`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the one hot encoding for a column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenate it to the current dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove the original column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "# since we already did encoding for \"Number in Season\", remove it from your list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write a loop to do it for the remaining columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the function from tutorial_utils to one-hot encode the writers\n",
    "from tutorial_utils import get_writers_one_hot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Discussion_ : What is different about this one-hot?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Task_ : Concatenate to `data` and drop the original column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalizing your data\n",
    "\n",
    "\n",
    "Normalizing your data is when you transform numerical features so that they are between 0 and 1, inclusively. \n",
    "\n",
    "![https://stats.stackexchange.com/questions/70801/how-to-normalize-data-to-0-1-range](figures/normalize.png)\n",
    "\n",
    "We do this because many machine learning algorithms rely on multiplying the features with a matrix of weights to get a value. If the number of views range from 5,000,000, to 20,000,000 while the rating range from 7.0 to 9.0, that large number of viewing will have a bigger impact on the model, even though we really just want the ratio of one episode's views to another.\n",
    "\n",
    "You can still have a good model without normalizing data. You will just have a harder (read: longer) time training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>is_jon</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"10\" valign=\"top\">1</th>\n",
       "      <th>500</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17825</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3666</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>594</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2802</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5743</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8615</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6805</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7082</th>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: []\n",
       "Index: [(1, 500), (1, 17825), (1, 3666), (1, 594), (1, 2802), (1, 29), (1, 5743), (1, 8615), (1, 6805), (1, 7082)]"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# normalize the data\n",
    "from sklearn import preprocessing\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "for column in list(data):\n",
    "    try:\n",
    "        x_scaled = min_max_scaler.fit_transform(data[column].to_frame())\n",
    "        scaled_column = pd.DataFrame(x_scaled, columns = [column])\n",
    "        data.drop([column], axis = 1, inplace = True)\n",
    "        data = pd.concat([data, scaled_column], axis = 1)   \n",
    "    except ValueError: #Can't normalize categorical or text features\n",
    "        pass\n",
    "\n",
    "data.tail(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Choose Your Target Characters\n",
    "\n",
    "Today, we want to focus on a binary classification, meaning that we want our model to only have to choose between two options. Therefore, instead of predicting who said a given line from a script, we will choose a target character and predict whether or not that character said a particular line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the name and counts for the 10 most common characters (ones with the most lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Task_ : Choose your target character. The more lines the better. Set `target_character= <name string>`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we must create the feature `<target_column>` that is `1` if the character is the target and `0` if it is not. This is the feature we will be trying to predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the target_column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering\n",
    "An important part of machine learning is deriving additional features about the data for the model to learn. This is called [feature engineering](https://towardsdatascience.com/feature-engineering-for-machine-learning-3a5e293a5114). You do this by looking at your data and thinking, \"what can i figure out that the machine may not?\"\n",
    "We will come back to parts of this section in order to improve our model. For now, we'll skip some."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deriving discrete or continuous features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a discrete feature that marks the number of words in the sentence for each feature. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deriving categorical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the features \"previous speaker\" and \"next speaker\" that tells the model \n",
    "# who spoke before this instance and who spoke after this instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"['Name'] not found in axis\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-134-6f52ec92b13d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# We no longer need the name column\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Name\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36mdrop\u001b[0;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[1;32m   3938\u001b[0m                                            \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3939\u001b[0m                                            \u001b[0mlevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minplace\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3940\u001b[0;31m                                            errors=errors)\n\u001b[0m\u001b[1;32m   3941\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3942\u001b[0m     @rewrite_axis_style_signature('mapper', [('copy', True),\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36mdrop\u001b[0;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[1;32m   3778\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32min\u001b[0m \u001b[0maxes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3779\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3780\u001b[0;31m                 \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_drop_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3781\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3782\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m_drop_axis\u001b[0;34m(self, labels, axis, level, errors)\u001b[0m\n\u001b[1;32m   3810\u001b[0m                 \u001b[0mnew_axis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3811\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3812\u001b[0;31m                 \u001b[0mnew_axis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3813\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0maxis_name\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnew_axis\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3814\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mdrop\u001b[0;34m(self, labels, errors)\u001b[0m\n\u001b[1;32m   4963\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0merrors\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'ignore'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4964\u001b[0m                 raise KeyError(\n\u001b[0;32m-> 4965\u001b[0;31m                     '{} not found in axis'.format(labels[mask]))\n\u001b[0m\u001b[1;32m   4966\u001b[0m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m~\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4967\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdelete\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: \"['Name'] not found in axis\""
     ]
    }
   ],
   "source": [
    "# We no longer need the name column\n",
    "data.drop([\"Name\"], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Task_ : Which of those new features should be one-hot encoded? Do it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding Features using Annotation\n",
    "\n",
    "_Annotation_ is the process of labeling data. When we talk about annotation at CallMiner, we are referring to human annotation -- where a person manually labels the data. This label can either be used as a feature or (more often), it is used as the target to predict.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add a column named sentiment on a random sample of rows, default to zero\n",
    "annotation_sample = data.sample(10, random_state = 666)\n",
    "annotation_sample[\"sentiment\"]= 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# annotate the sample\n",
    "for i, row in annotation_sample.iterrows():\n",
    "    sentence = annotation_sample.at[i, \"Sentence\"]\n",
    "    sentiment = input(f\"Is this negative sentiment (1 yes, 0 no): {sentence}\" )\n",
    "    annotation_sample.at[i, \"sentiment\"] = sentiment\n",
    "annotation_sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Discussions_ : What are the challenges with human annotation? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deriving features from text\n",
    "\n",
    "Models cannot process blocks of text the way humans can--text must be transformed into numbers. One way to do this is word embeddings (more on this in the afternoon). Another way to do it is to create features like `said_dragon` . `said_dragon = 1` if the word \"dragon\" is said in a turn and `said_dragon = 0` if it is not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Task_ : What words might be said in the Game of Thrones scripts that would distinguish one character from another? (Hint: \"dracarys\" is spelled \"d-r-a-c-a-r-y-s\"). Make a list of these word strings. Assign the list to the variable `target_words`. Create a column in `data` for each word in target_words called `f\"says_{target_word}\"`. The default value should be `0`".
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we'll create features for each of these words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Balance your data\n",
    "\n",
    "Balancing data is important because sometimes we have so few examples of one class that the model scores great if we just pretend it never happens. Consider the model predicting if it is Christmas yet:\n",
    "![Christmas](figures/Christmas.jpg)\n",
    "Since it is rarely Christmas, the model scores well if it guesses that it is never Christmas. There are two main ways to [handle unbalanced data](https://towardsdatascience.com/handling-imbalanced-datasets-in-deep-learning-f48407a0e758):\n",
    "1. Weight balancing: When we are training our model, we tell it that it is waaaaay more important that it gets the minority class right than the majority class.\n",
    "\n",
    "2. Oversampling/Undersampling: Replicating the data in your minority class or throwing out data in your majority class so that both are equally represented. The code below undersamples the majority class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffle your data\n",
    "data = data.sample(frac = 1).reset_index(drop = True)\n",
    "# have an even number of your target's dialogue and other dialogue\n",
    "data = data.groupby(target_column)\n",
    "data = data.apply(lambda x: x.sample(data.size().min()))\n",
    "len(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train-Test Split\n",
    "When training a machine learning model, you must split your data in at least 2 sets -- the training set and the testing set [(link)](https://towardsdatascience.com/train-validation-and-test-sets-72cb40cba9e7).\n",
    "The training set is used by the model to learn. The model has access to the labels in the training set. For example, if you are predicting if a call leads to a sale, the model is aware of which calls did and did not lead to a sale in the training set only. Therefore, it adjusts the parameter weights based on what calls it gets right/wrong (more on this later). \n",
    "\n",
    "\n",
    "The test set is used to evaluate a trained model. After using the training labels to adjust the model weights, we want to ensure that the model can still perform on other data points. Test sets can be used across models to compare performance.\n",
    "\n",
    "Depending on how you chose to train your model, you may want to use a validation set in addition to training and test. These are like practice tests. They provide unbiased evaluation of the model while it is still training.  The models we are using today use [cross validation](https://machinelearningmastery.com/k-fold-cross-validation/) so we only need a training set and a test set.\n",
    "\n",
    "In general, you want most of your data in the training set with small percents for test (and validation). The exact percents depend on the model, but we usually do a .8-.1-.1 or a .9-.1 split.\n",
    "![pic](figures/train-test.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# separate your target column from your other features\n",
    "# Naming your features X and your target y is standard naming protocol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into train and test data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is a predictive model anyway...?\n",
    "I'm about to throw a lot of definitions at you, inspired or directly quoted from [here](https://machinelearningmastery.com/gentle-introduction-to-predictive-modeling/).\n",
    "\n",
    "A model without a training algorithm is nothing. So let's start with the algorithm.\n",
    "The goal of a supervised learning algorithm is to take some data with a known relationship and encode those relationships in a way that a model can interpret for predictions....Notice that I never said \"rules\"...don't think of them as \"rules\"! The model contains the learned relationships. ![pic](figures/what_is_a_model.png)\n",
    "\n",
    "Once trained, the model is nothing but \"a handful of numbers and a way of using those numbers to relate input...to an output\". ![pic](figures/Make-Predictions.png)\n",
    "\n",
    "Models are not _smart_. Models do not _know things_. Models are really, really, good at linear algebra. \n",
    "\n",
    "\n",
    "There are a LOT of different models out there. These aren't even all of them: ![pic](figures/machine-learning-algorithms.png)\n",
    "\n",
    "These models are different because they use different algorithms to learn the relationships in the data. I wish we had time to go into all of them but we will primarily focus on logistic regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression\n",
    "Logistic regression is a classification algorithm that predicts the probability of discrete values (is jon/isn't jon, yes/no, sale/no sale) given the features from the data instance. Therefore the output for each data instance is a value between zero and one-- where one is 100% probability of the positive class. \n",
    "\n",
    "It is called logistic \"regression\" because it is trying to fit the data to the logit function ![pic](figures/logit.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### So what is it learning?\n",
    "The logistic regression algorithm is learning the relationship between all of the features and the probability for any given instance.\n",
    "The probability of an instance is calculated as ![pic](figures/LR_formula.png)\n",
    "\n",
    "i = an instance\n",
    "\n",
    "$P_{i}$ = the probability of the positive class given the features\n",
    "\n",
    "$1-P_{i}$ = the probability that it is not the positive class given features\n",
    "\n",
    "$\\alpha$ = the [bias coefficient](https://www.quora.com/What-does-the-bias-term-represent-in-logistic-regression) (don't worry about it)\n",
    "\n",
    "$\\beta_{k}$ = the weight of feature k\n",
    "\n",
    "$x_{k}$ = the value of feature k for instance i\n",
    "\n",
    "\n",
    "It will calculate $Z_{i}$ for all instances $i$ (aka all the data points) and then see how well it is doing using a loss function.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss Function\n",
    "\n",
    "When a logistic regression model is intialized all the weights ($\\beta s$) are randomly set. It does its best to calculate $Z_{i}$ for all ${i}$ and then needs to see how badly it did. A cost function is used to mathematically measure how far off the model is. \n",
    "Let's think of a sale/no sale model. Intuitively, we want to change the weights of our model a lot if it is confident that a call is a sale but it is actually not a sale. Similarly, it should change the weights a lot if it is confident a call is not a sale but it is. On the other hand, if the model isn't confident either way and just makes a best guess, we don't want to overly change it. To accomplish this idea, we use the following cost function (in this picture $Z_i$ is $h_\\theta$): ![pic](figures/cost_function.png)\n",
    "\n",
    "Notice how mathematically, it accomplishes just that. If the model is way off, the cost is much more. \n",
    "\n",
    "The algorithm sums the loss for all instances and send this information to the weights ($\\beta s$) through a process called [back propogation](https://brilliant.org/wiki/backpropagation/) (aka a lot of calculus <3 ). This process slowly adjusts the weights to _minimize the cost_ over time. Essentially, by minimizing the loss, you're making your model more correct. \n",
    "This process is similar to game of hot-cold. \"Warmer...colder...warmer...burning up....\". The difference between a game of hot-cold and training a model is that when playing a game, there is an exact location that we are in search of--when you find it, you win! We do not have an exact destination in mind when training a model. So how do we know when to stop?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### When is training complete\n",
    "Training is complete when one of three conditions is met:\n",
    "1. The weights are barely changing with each iteration\n",
    "\n",
    "\n",
    "2. The model has completed a maximum number of iterations (epochs) through the process\n",
    "    \n",
    "    \n",
    "3. Using your validation set, you see that your model is beginning to [overfit](https://towardsdatascience.com/what-are-overfitting-and-underfitting-in-machine-learning-a96b30864690)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a model\n",
    "Let's train our own logistic regression model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tune Hyperparameters\n",
    "\n",
    "Hyperparameters are settings in a model that can be adjusted to maximize a model's performance. The different hyperparameters to play with can be found [here](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the hyperparameters\n",
    "tol = .0005 # tolerance threshold\n",
    "class_weight = 'balanced' # this automatically weight balances instead of under sampling for balance\n",
    "max_iter = 500 # how many times to repeat the cost minimizing process before giving up\n",
    "random_state = 666 # a seed so that your weights are randomly initalized the same every time while refining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make an instance of a model and train it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate your model\n",
    "\n",
    "There are many ways that a model can be scored ([precision, recall, f1, accuracy](https://towardsdatascience.com/accuracy-precision-recall-or-f1-331fb37c5cb9), [AUC-ROC](https://towardsdatascience.com/understanding-auc-roc-curve-68b2303cc9c5), etc). We will be using average accuracy.\n",
    "\n",
    "This is calculated as $\\frac{NumberCorrectSamples}{ TotalSamples}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Is this too good to be true? Check the Confusion Matrix\n",
    "\n",
    "A confusion matrix is a matrix (duh..) that shows how the model is labeling instances on a class by class basis: ![pic](figures/confusion_matrix.png)\n",
    "\n",
    "The diagonal from top left to bottom right shows the number of instances guessed correctly, while the diagonal from the bottom left to top right shows the instances guessed in correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make confusion matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Task_ : Under \"Tuning Hyperparameters\", change `max_iter = 4`, `class_weight = 'None'`, and (if we got to it) comment out the code under \"Balance Your Data\". Rerun all cells above. What happened to your accuracy? What about the confusion matrix? Undo the changes and rerun it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Using the model for predictions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Probabilities\n",
    "Logistic Regression models use probabilities to predict if an instance is in the positive class. Once a logistic regression model is trained, we can pass instances to the model and it will return the probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get probabilities of the test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a sale/ no sale, this information would be extremely helpful. Instead of predicting \"this agent will make a sale\", you can predict \"this agent is 70.1% likely to make the sale\"....\n",
    "\n",
    "\n",
    "But what is the next thing that will be asked....."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Weights\n",
    "\n",
    "The next thing that most supervisors or C-levels or agents at risk of missing their bonus will ask is ..... \"WHY?\". Since logistic regression probabilities are calculated using a very clear relationship between features and their weights, it is fairly easy to understand what features play an important role in the decision by the model. It also helps with refining the model when you can see what is informative.\n",
    "\n",
    "Note that the feature weights only correspond to feature importance if each feature is *independent* of all other features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get weights "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Discussion_ : What does a negative weight mean? Which features would have the least impact on the model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Refine\n",
    "\n",
    "We've built a model..... now what? We make it better! Once a model is made, we continue the process of creating more features, tuning hyper parameters and evaluating. Eventually, we get the best model possible. Let's go back and implement any features in the \"Feature Engineering\" section we might have skipped"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Task_ : Make your model better! Some suggestions:\n",
    "1. Add more words / Remove words from your to your text features\n",
    "2. Derive more categorical or continuous features\n",
    "    1. The IMDB rating / number of views\n",
    "    2. The number of words in the previous/ next dialogue\n",
    "    3. The month the show aired\n",
    "3. Tune the hyper parameters -- check out the documentation if you want more control\n",
    "4. Try a different model [algorithm](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.linear_model )\n",
    "5. Try a non-binary classifier (i.e predicting jon, arya, or someone else)\n",
    "6. Adjust your train-test percentages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks\n",
    "![pic](figures/trained_a_neural_net.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What's the difference between logistic regression and a basic neural network\n",
    "Neural nets are made up of nodes and each node has a weight associated to it.\n",
    "In logistic regression, there is simply one node for each feature and these weights input directly into the decision. In neural nets, there are hidden layers of nodes between the feature nodes and the decision. These hidden nodes also have weights and can represent a variety of aspects (combinations of features, combinations of combinations of features, parts of multiple features....). The architect does not get to decide what theses hidden nodes represent-that is something the algorithm figures out ![pic](figures/lr_vs_nn.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### So what does the architect get to decide?\n",
    "Like with logistic regression, the architect gets to decide the hyperparameters. Most of the sort of hyperparameters set for logistic regression would also apply to a neural net. However, the architect must also decide:\n",
    "1. How many hidden layers\n",
    "2. How many nodes are in each hidden layer\n",
    "\n",
    "In more complex architectures, things like attention layers, convolution layers, and memory gates can also be added! Feel free to research more on your own! ![pic](figures/hidden_layers.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Common misconception about neural nets\n",
    "There are a lot of misconceptions about neural nets. The term is often thrown around like \"AI\". We want to take a moment to address a few of them\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Neural nets will take over the world\n",
    "![pic](figures/cat_as_dog.png)\n",
    "\n",
    "No, the end is not near. Like any other machine learning algorithm, neural nets can get very, very, good at specific tasks. However, they cannot \"reason\". They cannot branch out beyond what they are trained to do. They are also only as good as the data they are trained on. Therefore, there is little risk of a robot revolution any time soon."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Neural nets are a black box\n",
    "In the words of the Black-Eyed Peas, \"that's so 2000 and late\". In recent years, researchers have developed a variety of techniques to expose what each hidden node is focusing on. For example, researchers can determine what a convolutional neural network is focusing on when classifying images (hint: neural nets love edges and boundaries) ![pic](figures/inside_cnn.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Neural nets are the answer to everything\n",
    "![pic](figures/hammer.jpg)\n",
    "Not all problems require a neural net (or machine learning in general). If you simply want to know how certain features are correlated, there is no need to build a nn and then disect the weights--statistical methods can work just as well (if not better). If you are predicting home values, a linear regression might be enough! Lastly, there are still cases where rule-based approaches are the answer (we'll talk about that more with cluster labeling!). Before diving into building a neural net, make sure the problem statement requires it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A neural net is a specific thing\n",
    "A neural net is a type of architecture. Researchers are constantly developing new types of neural nets. Different types of neural nets thrive at different tasks (like CNNs for image classification). Picking the right type of neural net is a huge part of the job. ![pic](figures/NeuralNetworkZoo.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### If you want a neural net, you better build it from scratch\n",
    "Nowadays, there are many python packages that help you build neural nets. Some are simple (like the scikit learn ones we use today) while other packages are more complex but allow for more complex neural net architectures. There are also platforms like DataRobot that choose and make machine learning models for its users. So before you get nose-deep in calculus, find what already exists and utilize those tools. ![pic](figures/ml_packages.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training our own neural net: multi layered perceptron\n",
    "\n",
    "We will use scikit learn to create a [multilayer perceptron](http://deeplearning.net/tutorial/mlp.html).\n",
    "\n",
    "The main hyperparameters we must decide are how many hidden layers and the size of each hidden layer. For scikit learn, you create a tuple, where the length of your tuple is the number of hidden layers and the integer at tuple\\[i\\] is the size of the ith hidden layer. For example, if `hidden_layers = (32, 16, 8, 2)`, my model would have 4 hidden layers of size 32, 16, 8, and 2. The size of your last hidden layer should equal the number of classes. Since we are doing a binary classification, the size of last hidden layer should be 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tune hyperparameters\n",
    "solver= 'adam'\n",
    "hidden_layer_sizes = (32, 16, 8, 2)\n",
    "random_state = 666\n",
    "batch_size = 5 #instead of training on all your instances at once, the MLP trains in batches\n",
    "max_iter= 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make, train, and score your model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check out confusion matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Task_ : Refine this model like you did for the logistic regression. How accurate can it get? Which model performed better?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Homework\n",
    "A random sample of dialogue has been held out by us. Using only python code and packages, refine/create your best possible model to predict if a piece of dialogue is spoken by Tyrion Lannister. The person with the best performing model wins a prize to be determined (probably lunch). There will also be a prize for the best feature engineer. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Survey !!!\n",
    "\n",
    "Please complete the [course survey](https://forms.office.com/Pages/ResponsePage.aspx?id=gwv7BWBlfUGFbTjusOst_QYpnoW2nrtJmgVZLQ3gu25UMURGMDdaUTA0QUhJQTM3NlMxNE9GVVkyRC4u)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
