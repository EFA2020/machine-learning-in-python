{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Embedding Advanced Tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What's in-scope\n",
    "- improving our model: incorporating phraser\n",
    "- hyperparameters for training word embeddings\n",
    "    - the architecture: CBOW vs Skip-gram\n",
    "    - embedding window\n",
    "- different algorithms to train word embedding\n",
    "    - Word2Vec\n",
    "    - FastText\n",
    "    - GloVe\n",
    "    - BERT\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What's not in-scope\n",
    "- [ELMO](https://allennlp.org/elmo), another variation to train word embeddings\n",
    "- comparing and [evaluating word embedding models](https://arxiv.org/pdf/1901.09785.pdf) objectively"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intro\n",
    "There are a few different ways to train word embeddings. In the previous tutorial, we implemented FastText using gensim, but we didn't go into the detail of how it worked or why it worked well. \n",
    "\n",
    "In this tutorial, we'll dig into a few algorithms used in training word embeddings. We'll weight their strengths and weaknesses. \n",
    "\n",
    "First, let's incorporate one more step to improve the process: gensim's [phraser](https://radimrehurek.com/gensim/models/phrases.html). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phraser\n",
    "Some phrases have a meaning that is _not_ a simple composition of the meanings of its individual words. For example, `red head` or `book worm` don't mean exactly what the combination of their parts might imply. \n",
    "\n",
    "If we tried to get the meaning of `red head` by combining the vectors for `red` and `head`, would we get something that makes sense? `Red` is probably near other colors, `yellow`, `blue`, `green`. `Head` would be near body parts: `leg`, `arm`, `torso`. We'd want `red head` to be near `blond`, `blonde`, and `brunette`. \n",
    "\n",
    "![red-head](figures/red-head-vectors.png)\n",
    "\n",
    "In order to get this concept in the right space, we need to change our preprocessing step so that `red head` is treated as one unit, one word. That way, the vector for `red head` is trained separately from `red` and `head`. It's getting the context that `red head` falls into, unlike `red apple` or `red firetruck`. \n",
    "\n",
    "Since we're using FastText, we're also getting some information from the characters of the words, so the information from each piece of the phrase is still somewhat included. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we're going to modify our Sentences object from yesterday to incorporate phraser\n",
    "# but first, let's get familiar with gensim's phraser\n",
    "# (all snippets taken from gensim documentation)\n",
    "\n",
    "from gensim.models.phrases import Phraser, Phrases\n",
    "import os\n",
    "from gensim.test.utils import datapath\n",
    "from gensim.models.word2vec import Text8Corpus\n",
    "\n",
    "# gensim even gives you some toy data to use\n",
    "sentences = Text8Corpus(datapath('testcorpus.txt'))\n",
    "\n",
    "# The training corpus must be a sequence (stream, generator) of sentences,\n",
    "# with each sentence a list of tokens\n",
    "\n",
    "# print out a sentence so you know what it looks like \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a toy bigram model.\n",
    "phrases = Phrases(sentences, min_count=1, threshold=1)\n",
    "# Apply the trained phrases model to a new, unseen sentence.\n",
    "phrases[['trees', 'graph', 'minors']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The toy model considered \"trees graph\" a single phrase => joined the two\n",
    "# tokens into a single token, `trees_graph`.\n",
    "# Export the trained model = use less RAM, faster processing. Model updates no longer possible.\n",
    "bigram = Phraser(phrases)\n",
    "bigram[['trees', 'graph', 'minors']]  # apply the exported model to a sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How did it know which words were phrases and which weren't?\n",
    "What makes a combination of words a phrase?\n",
    "\n",
    "A phrase consists of words that \"appear frequently together, and infrequently in other contexts\" ([Mikolov, et al](https://arxiv.org/pdf/1310.4546.pdf)).\n",
    "\n",
    "So `New York Times` will be phrased, but not something with very frequently occurring terms like `this is`.\n",
    "\n",
    "Gensim implements this using this formula:\n",
    "![phraser score equation](figures/phraser-score-equation.png)\n",
    "\n",
    "So to get a high score, words need to occur very frequently together as a bigram, and far less frequently with other words.\n",
    "\n",
    "There's another scoring method that relies on mutual information:\n",
    "![npmi score](figures/phraser-score-npmi.png)\n",
    "\n",
    "Simple, right? \n",
    "\n",
    "Regardless of how we score the word combination, the score must be greater than the value for `threshold` to be considered a phrase."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some other options when running the Phraser\n",
    "\n",
    "#### Phrase Order\n",
    "You can also train the phrases model iteratively, passing in phrased data to another phrases model. After one run of phraser, you'll get bigrams. What's the max length of a phrase after using two phrases models?\n",
    "\n",
    "#### Common Terms\n",
    "You can also take stopwords into consideration when training a phrases model. \n",
    "\n",
    "Some phrases contain stopwords, words that hold very little meaning on their own. Something like `hold the phone` or `cat's got your tongue`. Gensim offers the ability to consider a list of supplied stopwords differently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's take a list of stopwords, taken from NLTK\n",
    "stopword_file = os.path.join('resources', 'stopwords.txt')\n",
    "\n",
    "'''\n",
    "------------------------------------------------\n",
    "Objective: get a set of stopwords from the file\n",
    "------------------------------------------------\n",
    "gensim.Phrases has a `common_terms` param that expects a set of strings\n",
    "\n",
    "This lets the phraser create longer phrases by handling the stopwords in\n",
    "this set differently than other vocab items\n",
    "\n",
    "the file in resources/stopwords.txt has one stopword per line\n",
    "\n",
    "he\n",
    "is\n",
    "me\n",
    "the\n",
    "...\n",
    "\n",
    "Read in this file, and get a set of strings (words) to use in the phraser\n",
    "'''\n",
    "# be sure to show a few words in the set to make sure it works as expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a new toy bigram model with stopwords\n",
    "phrases = Phrases(sentences, \n",
    "                  min_count=1, \n",
    "                  threshold=1,\n",
    "                  common_terms=stopwords_set)\n",
    "# Apply the trained phrases model to a new, unseen sentence.\n",
    "phrases[['computer', 'is', 'off']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what?? This still doesn't phrase? \n",
    "\n",
    "# let's look at sentences...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What's happening here?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "-------------------------------------------------------------------------\n",
    "Objective: modify the Sentences object from yesterday to phrase the data\n",
    "-------------------------------------------------------------------------\n",
    "\n",
    "Using your code from yesterday, modify the sentences object so that it yields \n",
    "phrased data.\n",
    "\n",
    "Add a function to your class called `create_phrasers` that trains the gensim\n",
    "phrases models and uses them to yield phrased data.\n",
    "''' \n",
    "from gensim.models.phrases import Phraser, Phrases\n",
    "import os\n",
    "import string\n",
    "\n",
    "class Sentences:\n",
    "    def __init__(self, filename, delim, encoding, limit=float('inf'), phrasers=None):\n",
    "        pass\n",
    "    \n",
    "    def create_phrasers(self, phrase_order=2, min_count=5, threshold=5):\n",
    "        pass\n",
    "        \n",
    "    def __iter__(self):\n",
    "        pass\n",
    "                \n",
    "got_dialogue_file = os.path.join('data', 'got_scripts_breakdown.csv')\n",
    "sents = Sentences(filename=got_dialogue_file, \n",
    "                  delim=';', \n",
    "                  encoding='utf-8-sig',\n",
    "                 )\n",
    "# check that your function for creating phrasers works\n",
    "sents.create_phrasers(phrase_order=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I only want to show a few sentences, \n",
    "# but I want to use the phrasers I've already trained\n",
    "# initialize a new sentences object with limit=5 and phrasers=sents.phrasers\n",
    "five_sentences = None\n",
    "# iterate through it and print the output to see what phrases occur\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How do phrasers change the FastText model?\n",
    "# retrain a fastText model using your newly phrased sentences object\n",
    "# to be consistent, use the same hyperparameters as yesterday\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# challenge: find phrases using most_similar\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's save this model for later use\n",
    "save_path = os.path.join('models', 'phrased_got_ft.model')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Important hyperparameters in training algorithms\n",
    "\n",
    "Regardless of the algorithm chosen, there are important hyperparameters to choose from.\n",
    "\n",
    "The word _architecture_ is used here to mean the internal structure of a neural network. \n",
    "\n",
    "Remember that the word embeddings are the weight matrix of the neural model used. \n",
    "\n",
    "![CBOW](figures/word2vec-cbow.png)\n",
    "![weight_matrix](figures/weight-matrix.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CBOW vs Skip-gram\n",
    "This [article](https://www.quora.com/What-are-the-continuous-bag-of-words-and-skip-gram-architectures) has the best distinction I've found.\n",
    ">These two architectures describe how the neural network \"learns\" the underlying word representations for each word. Since learning word representations is essentially unsupervised, you need some way to \"create\" labels to train the model. Skip-gram and CBOW are two ways of creating the \"task\" for the neural network -- you can think of this as the output layer of the neural network, where we create \"labels\" for the given input (which depends on the architecture).\n",
    ">\n",
    ">CBOW: The input to the model could be $w_{i-2}, w_{i-1}, w_{i+1}, w_{i+2}$, the preceding and following words of the current word we are at. The output of the neural network will be $𝑤_i$. Hence you can think of the task as \"predicting the word given its context\"\n",
    "Note that the number of words we use depends on your setting for the window size.\n",
    "\n",
    "![CBOW simple](figures/CBOW-simple.png)\n",
    "\n",
    "[image source](https://towardsdatascience.com/word-embeddings-intuition-and-some-maths-to-understand-end-to-end-skip-gram-model-cab57760c745)\n",
    "\n",
    "The sentence encoded in the above image is \"A cat ________ a mouse.\"\n",
    "\n",
    ">Skip-gram: The input to the model is $w_i$, and the output could be $w_{i-2}, w_{i-1}, w_{i+1}, w_{i+2}$. So the task here is \"predicting the context given a word\". In addition, more distant words are given less weight by randomly sampling them. \n",
    "\n",
    "![Skip-gram detailed](figures/Skip-gram-detailed.png)\n",
    "\n",
    "To summarize,\n",
    "\n",
    "CBOW predicts \"I went to the store to buy _________\"\n",
    "\n",
    "Skipgram predicts \"_______ _______ butter ________ _______ _______\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_How do I know when it's best to use CBOW or Skip-gram?_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And according to [Mikolov](https://en.wikipedia.org/wiki/Tomas_Mikolov),  \n",
    ">Skip-gram: works well with small amount of the training data, represents well even rare words or phrases.\n",
    ">\n",
    ">CBOW: several times faster to train than the skip-gram, slightly better accuracy for the frequent words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrain your FastText model using skipgram or CBOW using the parameter\n",
    "# sg=1 for true, 0 for CBOW\n",
    "# keep the same hyperparameters for consistency\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# explore the model to see any differences\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Did anything change? Speed of training?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you prefer this model, save it over the CBOW version\n",
    "# if not, keep the CBOW one\n",
    "save_path = os.path.join('models', 'phrased_got_ft.model')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Embedding Window\n",
    "\n",
    "Another hyperparameter shared across all algorithms and architectures is the embedding window. \n",
    "\n",
    "The embedding window determines how far to the right and left of a word to consider the context. \n",
    "\n",
    "Quiz: What is the embedding window for this image?\n",
    "\n",
    "![CBOW visualization](figures/CBOW-visualized.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Different Algorithms for training word embeddings\n",
    "\n",
    "There are a little more than a handful of algorithms used to train word embeddings.\n",
    "\n",
    "For all of these architectures, the two main questions are \n",
    "1. How is the data encoded?\n",
    "2. What is the architecture used in the model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec\n",
    "To train word embeddings using Word2Vec, as seen explicitly above in the images for CBOW and Skip-gram, we do the following for a fixed number of epochs.\n",
    "\n",
    "1. encode each word as a one-hot vector\n",
    "2. for each instance (or mini-batch) in the data set:\n",
    "    1. predict the missing words (according to CBOW or Skip Gram)\n",
    "    2. calculate the loss \n",
    "    3. use backpropagation and update the weights accordingly\n",
    "\n",
    "Here is the architecture of the neural network:\n",
    "![word2vec](figures/word2vec.png)\n",
    "\n",
    "Let's train a Word2Vec model using [gensim's documentation](https://radimrehurek.com/gensim/models/word2vec.html). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's use gensim to train a word2vec model\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "\n",
    "# save the model here\n",
    "model_save_path = os.path.join('models', 'got_w2v.model')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check it out with a most_similar call\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how about a word out of the model's vocabulary?\n",
    "# try \"bran_will_never_be_my_king\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "word2Vec doesn't have a way to handle words that it hasn't seen before in the training data set. It will throw an error when you ask for a word it hasn't seen before. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FastText\n",
    "\n",
    "The reason FastText achieves such incredible performance for word representations and sentence classification is in-part due to it's use of character level information.\n",
    "\n",
    "From [FastText: Under the Hood](https://towardsdatascience.com/fasttext-under-the-hood-11efc57b2b3)\n",
    ">Each word is represented as a bag of character n-grams in addition to the word itself, so for example, for the word `matter`, with $n = 3$, the FastText representations for the character n-grams is `<ma`, `mat`, `att`, `tte`, `ter`, `er>`. `<` and `>` are added as boundary symbols to distinguish the ngram of a word from a word itself. So for example, if the word `mat` is part of the vocabulary, it is represented as `<mat>`. \n",
    ">\n",
    ">This helps preserve the meaning of shorter words that may show up as ngrams of other words. Inherently, this also allows you to capture meaning for suffixes/prefixes.\n",
    "    \n",
    "The architecture is similar to that of Word2Vec, however some additional math changes are used. See the [the original paper](https://arxiv.org/pdf/1607.04606.pdf) for more detail."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Why is FastText good on our data? \n",
    "\n",
    "FastText is also useful on data sets with speech misrecognition errors. Speech recognition often gets consonants correct, but misses correct vowel sounds. Think `bowl` and `ball`. We get the `b` and the `l` in both, but miss the middle vowel sound characters. Because FastText accounts for these characters, its easier for aliases to be treated as similar words. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we've trained enough FastText models today :)\n",
    "# let's load in the one from before\n",
    "save_path = os.path.join('models', 'phrased_got_ft.model')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FastText is able to generate a vector for words it hasn't seen before\n",
    "# because of the character embeddings it employs\n",
    "# retry 'bran_will_never_be_my_king'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GloVe\n",
    "\n",
    "From [the GloVe website](https://nlp.stanford.edu/projects/glove/)\n",
    ">The GloVe model is trained on the non-zero entries of a global word-word co-occurrence matrix, which tabulates how frequently words co-occur with one another in a given corpus. Populating this matrix requires a single pass through the entire corpus to collect the statistics. For large corpora, this pass can be computationally expensive, but it is a one-time up-front cost. Subsequent training iterations are much faster because the number of non-zero matrix entries is typically much smaller than the total number of words in the corpus.\n",
    "\n",
    "Here's a co-occurrence matrix example:\n",
    "![co-occurrence matrix](figures/co-occurrence-matrix.png)\n",
    "\n",
    ">GloVe is essentially a log-bilinear model with a weighted least-squares objective. The main intuition underlying the model is the simple observation that ratios of word-word co-occurrence probabilities have the potential for encoding some form of meaning. For example, consider the co-occurrence probabilities for target words `ice` and `steam` with various probe words from the vocabulary. Here are some actual probabilities from a 6 billion word corpus:\n",
    "\n",
    "![probabilities](figures/GloVe.png)\n",
    "\n",
    "> As one might expect, `ice` co-occurs more frequently with `solid` than it does with `gas`, whereas `steam` co-occurs more frequently with `gas` than it does with `solid`. Both words co-occur with their shared property `water` frequently, and both co-occur with the unrelated word `fashion` infrequently. Only in the ratio of probabilities does noise from non-discriminative words like `water` and `fashion` cancel out, so that large values (much greater than 1) correlate well with properties specific to `ice`, and small values (much less than 1) correlate well with properties specific of `steam`. In this way, the ratio of probabilities encodes some crude form of meaning associated with the abstract concept of thermodynamic phase.\n",
    "\n",
    ">The training objective of GloVe is to learn word vectors such that their dot product equals the logarithm of the words' probability of co-occurrence. Owing to the fact that the logarithm of a ratio equals the difference of logarithms, this objective associates (the logarithm of) ratios of co-occurrence probabilities with vector differences in the word vector space. Because these ratios can encode some form of meaning, this information gets encoded as vector differences as well. For this reason, the resulting word vectors perform very well on word analogy tasks, such as those examined in the word2vec package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# could install another package to train GloVe\n",
    "# https://stackoverflow.com/questions/48962171/how-to-train-glove-algorithm-on-my-own-corpus\n",
    "\n",
    "# but let's leave this exercise for later"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BERT\n",
    "\n",
    "BERT stands for [Bidirectional Encoder Representations from Transformers](https://arxiv.org/pdf/1810.04805.pdf). This paper released by Google AI has become the state-of-the-art for many NLP tasks like question answering. \n",
    "\n",
    "From this [blog](https://towardsdatascience.com/bert-explained-state-of-the-art-language-model-for-nlp-f8b21a9b6270) (which I'll quote throughout this section):\n",
    "> BERT’s key technical innovation is applying the bidirectional training of Transformer, a popular attention model, to language modelling. This is in contrast to previous efforts which looked at a text sequence either from left to right or combined left-to-right and right-to-left training. The paper’s results show that a language model which is bidirectionally trained can have a deeper sense of language context and flow than single-direction language models. In the paper, the researchers detail a novel technique named Masked LM (MLM) which allows bidirectional training in models in which it was previously impossible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Attention? Transformer? What does all this mean?_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Foundationally, BERT relies on another paper by google, [Attention is All You Need](https://arxiv.org/pdf/1706.03762.pdf), which introduces the transformer architecture. Without getting into the math, the paper talks about attention. This is easy to visualize for machine translation tasks. \n",
    "\n",
    "![https://machinetalk.org/2019/03/29/neural-machine-translation-with-attention-mechanism/](figures/attention-machine-translation.gif)\n",
    "\n",
    "Here, the thickness of the lines connecting the words shows how much weight the model is putting on each English word when making the translation to the French word. Note that the model considers _all_ English words when making the translation, some with more importance than others. The model _attends_ to certain words more than others at each step.\n",
    "\n",
    "> As opposed to directional models, which read the text input sequentially (left-to-right or right-to-left), the Transformer encoder reads the entire sequence of words at once. Therefore it is considered bidirectional, though it would be more accurate to say that it’s non-directional. This characteristic allows the model to learn the context of a word based on all of its surroundings (left and right of the word)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Okay, neat, but weren't we making word embeddings? Not doing machine translation?_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BERT is making use of the the transformer architecture that learns contextual relations between words in a text. Let's get into the architecture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model architecture\n",
    "Input: sequence of tokens, embedded into vectors\n",
    "\n",
    "Output: sequence of vectors (size $H$) corresponding to an input token with the same index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training\n",
    "In architectures like Word2Vec, we had the option of training using CBOW or Skip-gram. These define the prediction goal. But they rely on a directionality, which limits context learning. BERT overcomes this using two strategies. \n",
    "\n",
    "##### Masked LM (MLM)\n",
    "\n",
    "> Before feeding word sequences into BERT, 15% of the words in each sequence are replaced with a [MASK] token. The model then attempts to predict the original value of the masked words, based on the context provided by the other, non-masked, words in the sequence. \n",
    "![MLM prediction from https://blog.insightdatascience.com/using-bert-for-state-of-the-art-pre-training-for-natural-language-processing-1d87142c29e7](figures/BERT-MLM-prediction.png)\n",
    ">In technical terms, the prediction of the output words requires:\n",
    ">\n",
    "> 1. Adding a classification layer on top of the encoder input\n",
    ">\n",
    "> 2. Multiplying the output vectors by the embedding matrix, transforming them into the vocabulary dimension\n",
    ">\n",
    "> 3. Calculating the probability of each word in the vocabulary with softmax.\n",
    "\n",
    "![MLM](figures/BERT-MLM.png)\n",
    "\n",
    "> The BERT loss function takes into consideration only the prediction of the masked values and ignores the prediction of the non-masked words. As a consequence, the model converges slower than directional models, a characteristic which is offset by its increased context awareness. \n",
    "\n",
    "(See the article for more detail, as this is a bit of a simplification)\n",
    "\n",
    "##### Next Sentence Prediction (NSP)\n",
    "\n",
    "> In the BERT training process, the model receives pairs of sentences as input and learns to predict if the second sentence in the pair is the subsequent sentence in the original document. During training, 50% of the inputs are a pair in which the second sentence is the subsequent sentence in the original document, while in the other 50% a random sentence from the corpus is chosen as the second sentence. The assumption is that the random sentence will be disconnected from the first sentence.\n",
    "\n",
    "![image from https://blog.insightdatascience.com/using-bert-for-state-of-the-art-pre-training-for-natural-language-processing-1d87142c29e7](figures/BERT-NSP-prediction.png)\n",
    "\n",
    ">\n",
    "> To help the model distinguish between the two sentences in training, the input is processed in the following way before entering the model:\n",
    ">\n",
    "> 1. A [CLS] token is inserted at the beginning of the first sentences and a [SEP] token is inserted at the end of each sentence.\n",
    "> \n",
    "> 2. A sentence embedding indicating Sentence A or Sentence B is added to each token. Sentence embeddings are similar in concept to token embeddings with a vocabulary of 2.\n",
    "> \n",
    "> 3. A positional embedding is added to each token to indicate its position in the sequence. The concept and implementation of positional embedding are presented in the Transformer paper. \n",
    "\n",
    "![encoding NSP](figures/BERT-NSP.png)\n",
    "\n",
    "> To help the model distinguish between the two sentences in training, the input is processed in the following way before entering the model:\n",
    "> \n",
    "> 1. The entire input sequence goes through the Transformer model.\n",
    "> \n",
    "> 2. The output of the [CLS] token is transformed into a $2x1$ shaped vector, using a simple classification layer (learned matrices of weights and biases). \n",
    "> \n",
    "> 3. Calculating the probability of $IsNextSequence$ with softmax.\n",
    "\n",
    "Combining these two strategies captures the context both from the sentence and from surrounding sentences. The goal of training is to minimize the combined loss functions of the two strategies. \n",
    "\n",
    "You can check out their implementation [here](https://github.com/google-research/bert). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# could train a BERT model following this tutorial\n",
    "# https://blog.insightdatascience.com/using-bert-for-state-of-the-art-pre-training-for-natural-language-processing-1d87142c29e7\n",
    "# but let's leave this exercise for another day..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary\n",
    "- phraser improves the word embedding model\n",
    "- hyperparameters for training word embeddings\n",
    "    - the architecture: CBOW vs Skip-gram\n",
    "    - embedding window\n",
    "- different algorithms to train word embedding\n",
    "    - Word2Vec\n",
    "    - FastText\n",
    "    - GloVe\n",
    "    - BERT\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Survey !!!\n",
    "\n",
    "Please complete the [course survey](https://forms.office.com/Pages/ResponsePage.aspx?id=gwv7BWBlfUGFbTjusOst_QYpnoW2nrtJmgVZLQ3gu25UMURGMDdaUTA0QUhJQTM3NlMxNE9GVVkyRC4u)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
